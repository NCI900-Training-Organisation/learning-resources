{
 "cells": [
  {
   "cell_type": "raw",
   "id": "52887d1c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Lucie - Modified Training\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb11c2a-8231-4ec8-8481-3a45823fc3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:10.223829Z",
     "iopub.status.busy": "2025-09-04T07:55:10.223264Z",
     "iopub.status.idle": "2025-09-04T07:55:10.510427Z",
     "shell.execute_reply": "2025-09-04T07:55:10.508636Z",
     "shell.execute_reply.started": "2025-09-04T07:55:10.223773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\u001b[m\n",
      " 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87aaf9b-ec92-4a30-8484-570a8d99f1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:10.587741Z",
     "iopub.status.busy": "2025-09-04T07:55:10.587153Z",
     "iopub.status.idle": "2025-09-04T07:55:14.390489Z",
     "shell.execute_reply": "2025-09-04T07:55:14.389095Z",
     "shell.execute_reply.started": "2025-09-04T07:55:10.587682Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import sys,os,time\n",
    "nb_dir=\"/g/data/dk92/notebooks/examples-aiml/lucie\"\n",
    "sys.path.append(f\"{nb_dir}/models\")\n",
    "from torch_harmonics_local_v2 import *\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa8476a-17f5-48dc-a18e-57fcd91bd938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:14.391880Z",
     "iopub.status.busy": "2025-09-04T07:55:14.391619Z",
     "iopub.status.idle": "2025-09-04T07:55:14.397068Z",
     "shell.execute_reply": "2025-09-04T07:55:14.396284Z",
     "shell.execute_reply.started": "2025-09-04T07:55:14.391861Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalization function used for static channels. \n",
    "def _minmax(img):\n",
    "        return torch.as_tensor((img-img.min())/(img.max()-img.min()))\n",
    "\n",
    "def generate_t30_grid():\n",
    "    nlat = 48  # Number of latitudes\n",
    "    nlon = 96  # Number of longitudes\n",
    "\n",
    "    # Gaussian latitudes and weights\n",
    "    latitudes, weights = np.polynomial.legendre.leggauss(nlat)\n",
    "    latitudes = np.arcsin(latitudes) * (180.0 / np.pi)  # Convert to degrees\n",
    "\n",
    "    # Longitudes\n",
    "    longitudes = np.linspace(0, 360, nlon, endpoint=False)\n",
    "\n",
    "    return latitudes, longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19764f27-3d6c-4e7b-a6c6-ff296c55bee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:14.397718Z",
     "iopub.status.busy": "2025-09-04T07:55:14.397564Z",
     "iopub.status.idle": "2025-09-04T07:55:14.411483Z",
     "shell.execute_reply": "2025-09-04T07:55:14.410020Z",
     "shell.execute_reply.started": "2025-09-04T07:55:14.397703Z"
    }
   },
   "outputs": [],
   "source": [
    "# modified inference function adapted to lat,lon inputs\n",
    "def inference(model, steps, initial_frame, forcing, initial_forcing_idx, prog_means, prog_stds, diag_means, diag_stds, diff_stds):\n",
    "    inf_data = []\n",
    "    inp_const =const_chans.to(device,dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inp_val = initial_frame\n",
    "        for i in range(steps):\n",
    "            forcing_idx = (initial_forcing_idx + i) % 1460      # tisr is repeating and orography is \n",
    "            previous = inp_val[:,:5,:,:]\n",
    "            inpc_val = torch.cat([inp_const, inp_val],dim=1)\n",
    "            pred = model(inpc_val)\n",
    "            pred[:,:5,:,:] = pred[:,:5,:,:] * diff_stds         # denormalize the predicted tendency\n",
    "            \n",
    "            # demornalzie the previous time step and add to the tendecy to reconstruct the current field\n",
    "            pred[:,:5,:,:] += previous[:,:5,:,:] * prog_stds + prog_means\n",
    "            \n",
    "            tp_frame = pred[:,5:,:,:] * diag_stds + diag_means\n",
    "            raw = torch.cat((pred[:,:5,:,:],tp_frame), 1)\n",
    "            \n",
    "            inp_val = (raw[:,:5,:,:] - prog_means) / prog_stds      # normalize the current time step for autoregressive prediction\n",
    "            inp_val = torch.cat((inp_val, forcing[forcing_idx,:,:,:].reshape(1,2,48,96)), dim=1)\n",
    "            raw = raw.cpu().clone().detach().numpy()\n",
    "            inf_data.append(raw[0])\n",
    "\n",
    "    inf_data = np.array(inf_data)\n",
    "    inf_data[:,5,:,:] = (np.exp(inf_data[:,5,:,:]) - 1) * 1e-2      # denormalzie precipitation that was normalized in log space\n",
    "    return inf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2636d330-0e7f-4072-87cb-efbcdb5fd410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:14.414900Z",
     "iopub.status.busy": "2025-09-04T07:55:14.414363Z",
     "iopub.status.idle": "2025-09-04T07:55:14.446432Z",
     "shell.execute_reply": "2025-09-04T07:55:14.445259Z",
     "shell.execute_reply.started": "2025-09-04T07:55:14.414847Z"
    }
   },
   "outputs": [],
   "source": [
    "# define util functions, overide the one defined in LUCIE_train.py\n",
    "def integrate_grid(ugrid):\n",
    "    dlon = 2 * torch.pi / nlon\n",
    "    out = torch.sum(ugrid * quad_weights * dlon, dim=(-2, -1))\n",
    "    return out\n",
    "\n",
    "def l2loss_sphere(prd, tar, relative=False, squared=True):\n",
    "    loss = integrate_grid((prd - tar)**2).sum(dim=-1)\n",
    "    if relative:\n",
    "        loss = loss / integrate_grid(tar**2).sum(dim=-1)\n",
    "\n",
    "    if not squared:\n",
    "        loss = torch.sqrt(loss)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_model(model, tdl, optimizer, scheduler=None, nepochs=20, loss_fn='l2'):\n",
    "    infer_bias = 1e+80\n",
    "    ibs = torch.zeros(1,nepochs)\n",
    "    best_bias = 1e+80\n",
    "    recall_count = 0\n",
    "    acc_losses = []\n",
    "    epoch_times = []\n",
    "    #ckpt_dir=f\"{os.environ['PBS_O_WORKDIR']}/checkpoints/{os.environ['PBS_JOBID']}\"\n",
    "    for epoch in range(nepochs):\n",
    "        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n",
    "        print(f'--------------------------------------------------------------------------------')\n",
    "        print(f\"{tstamp}: epoch {epoch} start\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        if epoch < 149:\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                print(f'using scheduler: current learning rate = {scheduler.get_last_lr()}')\n",
    "        else:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 1e-6\n",
    "                \n",
    "            print(f\"current learning rate = {optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        acc_loss = 0\n",
    "        model.train()\n",
    "        #batch_num = 0\n",
    "        for inp, tar in tdl:\n",
    "            #batch_num += 1\n",
    "            #loss = 0\n",
    "\n",
    "            #inp = inp.to(device)\n",
    "            # adding lat, lon as the first two channels\n",
    "            inpc = torch.cat([const_chans_t, inp],dim=1).to(device,dtype=torch.float32)\n",
    "            tar = tar.to(device)\n",
    "            prd = model(inpc)\n",
    "\n",
    "            # prd and tar shape are not affected, keep this section\n",
    "            loss_delta = l2loss_sphere(prd[:,:5,:,:], tar[:,:5,:,:], relative=True)\n",
    "            loss_tp = torch.mean((prd[:,5:,:,:]-tar[:,5:,:,:])**2)\n",
    "            loss = loss_delta + loss_tp / tar.shape[1]\n",
    "\n",
    "            if epoch > 150:\n",
    "                #print(f\"add spectral loss\")\n",
    "                lat_index = np.r_[7:15, 32:40]\n",
    "                out_fft = torch.mean(torch.abs(torch.fft.rfft(prd[:,:,lat_index,:],dim=3)),dim=2)\n",
    "                target_fft = torch.mean(torch.abs(torch.fft.rfft(tar[:,:,lat_index,:],dim=3)),dim=2)\n",
    "                loss_reg = 0.05 * torch.mean(torch.abs(out_fft - target_fft))\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc_loss += loss.item()* inp.size(0)\n",
    "            \n",
    "        acc_losses.append(acc_loss / len(tdl.dataset))\n",
    "\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n",
    "        print(f'{tstamp}: Epoch {epoch} summary:')\n",
    "        print(f'time taken: {epoch_times[-1]}')\n",
    "        print(f'nsamples / sec: {len(tdl.dataset)/epoch_times[-1]}')\n",
    "        print(f'average training loss: {acc_losses[-1]}')\n",
    "\n",
    "        if epoch >= 60:\n",
    "            rollout_steps = 2920\n",
    "            rollout = torch.tensor(inference(model, rollout_steps, data_inp[0:1].to(device), data_inp[:1460,-2:].to(device), 1, prog_means, prog_stds, diag_means, diag_stds, diff_stds)).to(device)\n",
    "            rollout_clim = torch.mean(rollout[1460:],dim=0)\n",
    "            clim_bias = torch.mean(torch.abs(rollout_clim - true_clim))\n",
    "            ibs[0,epoch] = clim_bias\n",
    "            if len(ibs>0)<=20:\n",
    "                infer_bias = torch.mean(torch.tensor(ibs[0,60:epoch+1]))\n",
    "            else:\n",
    "                infer_bias = torch.mean(ibs[0,epoch-20:epoch+1])\n",
    "\n",
    "            print(f'clim_bias: {clim_bias}')\n",
    "            print(f'infer_bias: {infer_bias}')\n",
    "            if clim_bias <= best_bias:\n",
    "                print(f\"new best clim_bias, save checkpoint\")\n",
    "                best_bias = clim_bias\n",
    "                #torch.save({\"epoch\":epoch,\"model_state_dict\":model.state_dict(),\"optim_state_dict\":optimizer.state_dict(),\"sch_state_dict\":scheduler.state_dict()},f\"{ckpt_dir}/lucie_{epoch}.pt\")\n",
    "                torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "         \n",
    "            if epoch % 10 == 0:\n",
    "                if ~torch.isnan(clim_bias): \n",
    "                    if clim_bias <= infer_bias:\n",
    "                        #print(f\"clim_bias <= {infer_bias}, save checkpoint\")\n",
    "                        #infer_bias = clim_bias\n",
    "                        #torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "                        recall_count = 0\n",
    "                    else:\n",
    "                        print(f\"clim_bias > {infer_bias}, recall from latest checkpoint\")\n",
    "                        state_pth = torch.load(f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "                        model.load_state_dict(state_pth)\n",
    "                        recall_count += 1\n",
    "                        if recall_count > 3:\n",
    "                            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe1310a-23a0-49f2-964f-28f037a5b846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:14.447491Z",
     "iopub.status.busy": "2025-09-04T07:55:14.447244Z",
     "iopub.status.idle": "2025-09-04T07:55:27.638255Z",
     "shell.execute_reply": "2025-09-04T07:55:27.636815Z",
     "shell.execute_reply.started": "2025-09-04T07:55:14.447466Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\n",
    "true_clim = torch.tensor(np.mean(data, axis=0)).to(device).permute(2,0,1)\n",
    "\n",
    "data = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")     # standardized data with mean and stds generated from dataset_generator.py\n",
    "data_inp = torch.tensor(data[\"data_inp\"],dtype=torch.float32)     # input data \n",
    "data_tar = torch.tensor(data[\"data_tar\"],dtype=torch.float32)\n",
    "raw_means = torch.tensor(data[\"raw_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "raw_stds = torch.tensor(data[\"raw_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "prog_means = raw_means[:,:5]\n",
    "prog_stds = raw_stds[:,:5]\n",
    "diag_means = torch.tensor(data[\"diag_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diag_stds = torch.tensor(data[\"diag_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diff_means = torch.tensor(data[\"diff_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diff_stds = torch.tensor(data[\"diff_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "\n",
    "ntrain = 16000\n",
    "train_set = TensorDataset(data_inp[:ntrain],data_tar[:ntrain])\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n",
    "\n",
    "# generate the const channels\n",
    "lats, lons = generate_t30_grid()\n",
    "lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "const_chans = _minmax(np.stack([_minmax(lat2d), _minmax(lon2d)])).unsqueeze(0)\n",
    "const_chans_t = const_chans.expand(16, 2, 48, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecae7d22-0ec0-481e-bc66-1288e36887e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:27.639348Z",
     "iopub.status.busy": "2025-09-04T07:55:27.639157Z",
     "iopub.status.idle": "2025-09-04T07:55:28.696475Z",
     "shell.execute_reply": "2025-09-04T07:55:28.695596Z",
     "shell.execute_reply.started": "2025-09-04T07:55:27.639330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3791376\n"
     ]
    }
   ],
   "source": [
    "# set the model\n",
    "nlat = 48\n",
    "nlon = 96\n",
    "hard_thresholding_fraction = 0.9\n",
    "cost, quad_weights = legendre_gauss_weights(nlat, -1, 1)\n",
    "quad_weights = (torch.as_tensor(quad_weights).reshape(-1, 1)).to(device)\n",
    "model = SphericalFourierNeuralOperatorNet(params = {}, spectral_transform='sht', filter_type = \"linear\", operator_type='dhconv', img_shape=(48, 96),num_layers=8, in_chans=9, out_chans=6, scale_factor=1, embed_dim=72, activation_function=\"silu\", big_skip=True, pos_embed=False, use_mlp=True,normalization_layer=\"instance_norm\", hard_thresholding_fraction=hard_thresholding_fraction,mlp_ratio = 2.).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-5)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e7bd4ae-84ce-4447-bbf6-8ad9f5713cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T07:55:28.697530Z",
     "iopub.status.busy": "2025-09-04T07:55:28.697229Z",
     "iopub.status.idle": "2025-09-04T07:59:13.204401Z",
     "shell.execute_reply": "2025-09-04T07:59:13.203528Z",
     "shell.execute_reply.started": "2025-09-04T07:55:28.697511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "17:55:28: epoch 0 start\n",
      "using scheduler: current learning rate = [9.999013075636805e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:44: Epoch 0 summary:\n",
      "time taken: 75.56519341468811\n",
      "nsamples / sec: 211.73769664288284\n",
      "average training loss: 0.5649356337823864\n",
      "--------------------------------------------------------------------------------\n",
      "17:56:44: epoch 1 start\n",
      "using scheduler: current learning rate = [9.996052735444863e-05]\n",
      "17:57:58: Epoch 1 summary:\n",
      "time taken: 74.5183458328247\n",
      "nsamples / sec: 214.71222718623653\n",
      "average training loss: 0.4445175377408696\n",
      "--------------------------------------------------------------------------------\n",
      "17:57:58: epoch 2 start\n",
      "using scheduler: current learning rate = [9.991120277927221e-05]\n",
      "17:59:13: Epoch 2 summary:\n",
      "time taken: 74.4180178642273\n",
      "nsamples / sec: 215.00169527749802\n",
      "average training loss: 0.41059033054741506\n"
     ]
    }
   ],
   "source": [
    "# it takes roughly 75 sec per epoch to train the modified LUCIE model on a single V100\n",
    "# checkpoints saved at epoch 193 and 219 are available in `/g/data/dk92/notebooks/examples-aiml/lucie/checkpoints/`\n",
    "nepochs=3\n",
    "train_model(model, train_loader, optimizer, scheduler=scheduler, nepochs=nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42694e8-3fa7-466b-8b38-bb23f60e7c7e",
   "metadata": {},
   "source": [
    "# Suggested Exercises\n",
    "1. identify the difference between the above `model` and the `model` used in `/g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb`.\n",
    "2. compare learning curve of this training and the one defined in `/g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb`.\n",
    "3. compare rollouts generated from this model and the one trained in `/g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb`.\n",
    "4. modify the model further to have higher embedding dimension in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca88b01-46c3-4a82-a6c6-99d723da24be",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
