{
 "cells": [
  {
   "cell_type": "raw",
   "id": "27e3fb11",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Lucie - Replicate Training\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb11c2a-8231-4ec8-8481-3a45823fc3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:20.296507Z",
     "iopub.status.busy": "2025-09-03T08:39:20.295885Z",
     "iopub.status.idle": "2025-09-03T08:39:20.577697Z",
     "shell.execute_reply": "2025-09-03T08:39:20.575909Z",
     "shell.execute_reply.started": "2025-09-03T08:39:20.296432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\u001b[m\n",
      " 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87aaf9b-ec92-4a30-8484-570a8d99f1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:20.580735Z",
     "iopub.status.busy": "2025-09-03T08:39:20.580159Z",
     "iopub.status.idle": "2025-09-03T08:39:22.611608Z",
     "shell.execute_reply": "2025-09-03T08:39:22.610070Z",
     "shell.execute_reply.started": "2025-09-03T08:39:20.580676Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import sys,os,time\n",
    "nb_dir=\"/g/data/dk92/notebooks/examples-aiml/lucie\"\n",
    "sys.path.append(f\"{nb_dir}/models\")\n",
    "from torch_harmonics_local import *\n",
    "from LUCIE_inference import inference\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2636d330-0e7f-4072-87cb-efbcdb5fd410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:22.612931Z",
     "iopub.status.busy": "2025-09-03T08:39:22.612662Z",
     "iopub.status.idle": "2025-09-03T08:39:22.627255Z",
     "shell.execute_reply": "2025-09-03T08:39:22.626503Z",
     "shell.execute_reply.started": "2025-09-03T08:39:22.612911Z"
    }
   },
   "outputs": [],
   "source": [
    "# define util functions, overide the one defined in LUCIE_train.py\n",
    "def integrate_grid(ugrid):\n",
    "    dlon = 2 * torch.pi / nlon\n",
    "    out = torch.sum(ugrid * quad_weights * dlon, dim=(-2, -1))\n",
    "    return out\n",
    "\n",
    "def l2loss_sphere(prd, tar, relative=False, squared=True):\n",
    "    loss = integrate_grid((prd - tar)**2).sum(dim=-1)\n",
    "    if relative:\n",
    "        loss = loss / integrate_grid(tar**2).sum(dim=-1)\n",
    "\n",
    "    if not squared:\n",
    "        loss = torch.sqrt(loss)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_model(model, tdl, optimizer, scheduler=None, nepochs=20, loss_fn='l2'):\n",
    "    infer_bias = 1e+80\n",
    "    ibs = torch.zeros(1,nepochs)\n",
    "    best_bias = 1e+80\n",
    "    recall_count = 0\n",
    "    acc_losses = []\n",
    "    epoch_times = []\n",
    "    #ckpt_dir=f\"{os.environ['PBS_O_WORKDIR']}/checkpoints/{os.environ['PBS_JOBID']}\"\n",
    "    ckpt_dir=os.getcwd()\n",
    "    for epoch in range(nepochs):\n",
    "        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n",
    "        print(f'--------------------------------------------------------------------------------')\n",
    "        print(f\"{tstamp}: epoch {epoch} start\")\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        if epoch < 149:\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                print(f'using scheduler: current learning rate = {scheduler.get_last_lr()}')\n",
    "        else:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 1e-6\n",
    "\n",
    "            print(f\"current learning rate = {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        acc_loss = 0\n",
    "        model.train()\n",
    "        #batch_num = 0\n",
    "        for inp, tar in tdl:\n",
    "            #batch_num += 1\n",
    "            #loss = 0\n",
    "\n",
    "            inp = inp.to(device)\n",
    "            tar = tar.to(device)\n",
    "            prd = model(inp)\n",
    "\n",
    "            loss_delta = l2loss_sphere(prd[:,:5,:,:], tar[:,:5,:,:], relative=True)\n",
    "            loss_tp = torch.mean((prd[:,5:,:,:]-tar[:,5:,:,:])**2)\n",
    "            loss = loss_delta + loss_tp / tar.shape[1]\n",
    "\n",
    "            if epoch > 150:\n",
    "                #print(f\"add spectral loss\")\n",
    "                lat_index = np.r_[7:15, 32:40]\n",
    "                out_fft = torch.mean(torch.abs(torch.fft.rfft(prd[:,:,lat_index,:],dim=3)),dim=2)\n",
    "                target_fft = torch.mean(torch.abs(torch.fft.rfft(tar[:,:,lat_index,:],dim=3)),dim=2)\n",
    "                loss_reg = 0.05 * torch.mean(torch.abs(out_fft - target_fft))\n",
    "                loss = loss + loss_reg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc_loss += loss.item()* inp.size(0)\n",
    "\n",
    "        acc_losses.append(acc_loss / len(tdl.dataset))\n",
    "\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n",
    "        print(f'{tstamp}: Epoch {epoch} summary:')\n",
    "        print(f'time taken: {epoch_times[-1]}')\n",
    "        print(f'nsamples / sec: {len(tdl.dataset)/epoch_times[-1]}')\n",
    "        print(f'average training loss: {acc_losses[-1]}')\n",
    "\n",
    "        if epoch >= 60:\n",
    "            rollout_steps = 2920\n",
    "            rollout = torch.tensor(inference(model, rollout_steps, data_inp[0:1].to(device), data_inp[:1460,-2:].to(device), 1, prog_means, prog_stds, diag_means, diag_stds, diff_stds)).to(device)\n",
    "            rollout_clim = torch.mean(rollout[1460:],dim=0)\n",
    "            clim_bias = torch.mean(torch.abs(rollout_clim - true_clim))\n",
    "            ibs[0,epoch] = clim_bias\n",
    "            if len(ibs>0)<=20:\n",
    "                infer_bias = torch.mean(torch.tensor(ibs[0,60:epoch+1]))\n",
    "            else:\n",
    "                infer_bias = torch.mean(ibs[0,epoch-20:epoch+1])\n",
    "\n",
    "            print(f'clim_bias: {clim_bias}')\n",
    "            print(f'infer_bias: {infer_bias}')\n",
    "            if clim_bias <= best_bias:\n",
    "                print(f\"new best clim_bias, save checkpoint\")\n",
    "                best_bias = clim_bias\n",
    "                #torch.save({\"epoch\":epoch,\"model_state_dict\":model.state_dict(),\"optim_state_dict\":optimizer.state_dict(),\"sch_state_dict\":scheduler.state_dict()},f\"{ckpt_dir}/lucie_{epoch}.pt\")\n",
    "                torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                if ~torch.isnan(clim_bias):\n",
    "                    if clim_bias <= infer_bias:\n",
    "                        #print(f\"clim_bias <= {infer_bias}, save checkpoint\")\n",
    "                        #infer_bias = clim_bias\n",
    "                        #torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "                        recall_count = 0\n",
    "                    else:\n",
    "                        print(f\"clim_bias > {infer_bias}, recall from latest checkpoint\")\n",
    "                        state_pth = torch.load(f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n",
    "                        model.load_state_dict(state_pth)\n",
    "                        recall_count += 1\n",
    "                        if recall_count > 3:\n",
    "                            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe1310a-23a0-49f2-964f-28f037a5b846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:22.627977Z",
     "iopub.status.busy": "2025-09-03T08:39:22.627807Z",
     "iopub.status.idle": "2025-09-03T08:39:35.149699Z",
     "shell.execute_reply": "2025-09-03T08:39:35.147910Z",
     "shell.execute_reply.started": "2025-09-03T08:39:22.627960Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\n",
    "true_clim = torch.tensor(np.mean(data, axis=0)).to(device).permute(2,0,1)\n",
    "\n",
    "data = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")     # standardized data with mean and stds generated from dataset_generator.py\n",
    "data_inp = torch.tensor(data[\"data_inp\"],dtype=torch.float32)     # input data \n",
    "data_tar = torch.tensor(data[\"data_tar\"],dtype=torch.float32)\n",
    "raw_means = torch.tensor(data[\"raw_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "raw_stds = torch.tensor(data[\"raw_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "prog_means = raw_means[:,:5]\n",
    "prog_stds = raw_stds[:,:5]\n",
    "diag_means = torch.tensor(data[\"diag_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diag_stds = torch.tensor(data[\"diag_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diff_means = torch.tensor(data[\"diff_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "diff_stds = torch.tensor(data[\"diff_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n",
    "\n",
    "ntrain = 16000\n",
    "train_set = TensorDataset(data_inp[:ntrain],data_tar[:ntrain])\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecae7d22-0ec0-481e-bc66-1288e36887e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:35.151272Z",
     "iopub.status.busy": "2025-09-03T08:39:35.151066Z",
     "iopub.status.idle": "2025-09-03T08:39:35.913334Z",
     "shell.execute_reply": "2025-09-03T08:39:35.912486Z",
     "shell.execute_reply.started": "2025-09-03T08:39:35.151253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4122864\n"
     ]
    }
   ],
   "source": [
    "# set the model\n",
    "nlat = 48\n",
    "nlon = 96\n",
    "hard_thresholding_fraction = 0.9\n",
    "cost, quad_weights = legendre_gauss_weights(nlat, -1, 1)\n",
    "quad_weights = (torch.as_tensor(quad_weights).reshape(-1, 1)).to(device)\n",
    "model = SphericalFourierNeuralOperatorNet(params = {}, spectral_transform='sht', filter_type = \"linear\", operator_type='dhconv', img_shape=(48, 96),\n",
    "                    num_layers=8, in_chans=7, out_chans=6, scale_factor=1, embed_dim=72, activation_function=\"silu\", big_skip=True, pos_embed=\"latlon\", use_mlp=True,\n",
    "                                            normalization_layer=\"instance_norm\", hard_thresholding_fraction=hard_thresholding_fraction,\n",
    "                                            mlp_ratio = 2.).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-5)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7bd4ae-84ce-4447-bbf6-8ad9f5713cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T08:39:35.914336Z",
     "iopub.status.busy": "2025-09-03T08:39:35.914031Z",
     "iopub.status.idle": "2025-09-03T08:43:15.155914Z",
     "shell.execute_reply": "2025-09-03T08:43:15.154935Z",
     "shell.execute_reply.started": "2025-09-03T08:39:35.914317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "18:39:35: epoch 0 start\n",
      "using scheduler: current learning rate = [9.998026259498021e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:990: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:40:49: Epoch 0 summary:\n",
      "time taken: 73.2493634223938\n",
      "nsamples / sec: 218.43193240787235\n",
      "average training loss: 0.568795047662941\n",
      "--------------------------------------------------------------------------------\n",
      "18:40:49: epoch 1 start\n",
      "using scheduler: current learning rate = [9.993093369094605e-05]\n",
      "18:42:02: Epoch 1 summary:\n",
      "time taken: 73.02629399299622\n",
      "nsamples / sec: 219.0991644945658\n",
      "average training loss: 0.44222918830883634\n",
      "--------------------------------------------------------------------------------\n",
      "18:42:02: epoch 2 start\n",
      "using scheduler: current learning rate = [9.986190524833163e-05]\n",
      "18:43:15: Epoch 2 summary:\n",
      "time taken: 72.95869874954224\n",
      "nsamples / sec: 219.30215689462784\n",
      "average training loss: 0.4068589104065609\n"
     ]
    }
   ],
   "source": [
    "# it takes roughly 75 sec per epoch to train the original LUCIE model on a single V100\n",
    "# checkpoint saved at epoch 340 is available in `/g/data/dk92/notebooks/examples-aiml/lucie/checkpoints/`\n",
    "nepochs=3\n",
    "train_model(model, train_loader, optimizer, scheduler=scheduler, nepochs=nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42694e8-3fa7-466b-8b38-bb23f60e7c7e",
   "metadata": {},
   "source": [
    "# Suggested Exercises\n",
    "1. plot the learning curve of the training: y-axis: average training loss, x-axis: epoch.\n",
    "2. modify `rollout_clim` and `true_clim` to be the latitude-weighted global mean and re-run the training\n",
    "3. try expand the region where the added spectral loss covers and re-run the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca88b01-46c3-4a82-a6c6-99d723da24be",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
