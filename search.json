[
  {
    "objectID": "tutorials/logingadi.html",
    "href": "tutorials/logingadi.html",
    "title": "Login to Gadi",
    "section": "",
    "text": "Access to Gadi is via SSH to gadi.nci.org.au. This provides a Unix shell on one of the Gadi login nodes.\nFor security reasons we ask that you avoid setting up passwordless ssh to Gadi. Entering your password every time you login is more secure, or using specialised ssh secure agents.\n\nConnecting from Linux/Mac/Unix\nOpen a Terminal application and use the following commands:\n ssh [your-6-character-username]@gadi.nci.org.au\nThe username is assigned when you created your NCI account and used for all HPC system accesses.\nNote: Do not use these login nodes to run long jobs or jobs with big computational requirements.\n\n\nConnecting from Windows\nThe hostname for SSH connections to Gadi is gadi.nci.org.au. Your username is provided when you created your NCI account. You can check it via Mancini at https://my.nci.org.au/.\nWindows does not provide an SSH client by default. We recommend:\nMobaXterm from http://mobaxterm.mobatek.net PuTTY from http://www.chiark.greenend.org.uk/~sgtatham/putty/ With MobaXterm on Windows system, create a new SSH session by clicking on Session tab or Sessions menu item on the top-left corner. Use gadi.nci.org.au as Remote host and NCI username as Specify username. Then log in with the new SSH session.\nNote: We advise that you make yourself familiar with the Linux operating system and the use of command line before continue using Gadi.\n\n\nRecomended learning: Introduction to the Unix Shell",
    "crumbs": [
      "Home",
      "Tutorials",
      "Login to Gadi"
    ]
  },
  {
    "objectID": "tutorials/filetransfer.html",
    "href": "tutorials/filetransfer.html",
    "title": "File Transfer",
    "section": "",
    "text": "File Transfer\nFile transfer should be done using Gadi’s data-mover nodes (domain name ‘gadi-dm.nci.org.au’), which are dedicated for moving data to and from the system at a high speed.\n\n\n\n\n\n\n\n\nOperating System\nProgram\nDescription\n\n\n\n\nMac and Linux\nrsync\nA powerful tool for transferring large files, with the ability to compare source and destination and only send changes.\n\n\nMac and Linux\nscp\nA simple file transfer tool (secure copy).\n\n\nWindows\nWinSCP\nA graphical interface for transferring files.\n\n\n\nNote: Transferring Code\nThe best way to transfer code from one computer to another is to host the code in a source code repository using a versioning system such as git and clone the repository to the supercomputer. Recommended learning: Version Control with Git\n\n\nSCP\nFrom local system → Gadi\nTo start a scp transfer through Gadi’s data-mover nodes, you can run the command,\n$ scp &lt;source&gt; &lt;destination&gt;\nNotice that the domain is gadi-dm.nci.org.au, as you are logging into a data-mover node instead of a login node.\nReplace the filename with the file you wish to transfer, including the file extension e.g. .sh, .pdf, and use your username as the login. The destination should be replaced with location that you want the file to be placed in, like the following for an example:\n$ scp testfile.sh &lt;user&gt;@gadi-dm.nci.org.au:/scratch/&lt;project dir&gt;/&lt;somewhere&gt;/\nYou will then be prompted for your password, once entered, your file transfer will begin.\nFrom Gadi → local system\nIf you want to do the opposite, move a file from Gadi to your local system, you simply need to reverse the prompt to reflect this, like so\n$ scp &lt;user&gt;@gadi-dm.nci.org.au:/scratch/&lt;project dir&gt;/&lt;somewhere&gt;/ ./\nTo find a listing of all the options available to you with scp, e.g. which option to use to transfer a folder/directory, use the command\n$ man scp\n\n\nrsync\nIf you are transferring a larger file and want to add a layer of protection to the transfer, using rsync will give you the ability to resume a transfer that has been interrupted. As with SCP, reversing your pathways will change which direction the transfer is happening, either uploading from your local device or downloading from Gadi.\nrsync also allows greater control over the transfer and exactly what happens with the file by placing options into the command line, such as\n$ rsync -vP  testfile.sh &lt;user&gt;@gadi-dm.nci.org.au:/home/900/&lt;user&gt;\nthe -vP in the command line means that the output file will be verbose and preserve all of its attributes such as timestamps. -P is a great way to get an exact copy of your file.\nTo find a listing of all the options available to you with rsync, e.g. which option to use to transfer a folder/directory, use the command\n$ man rsync\nto print a list of all of the variables that you can use.\n-a, --archive    archive mode; equals -rlptgoD (no -H,-A,-X)\n-v, --verbose               increase verbosity\n-P                          same as --partial --progress\n-x, --one-file-system       don't cross filesystem boundaries\n-H, --hard-links            preserve hard links\n-g, --group                 preserve group\n--chmod=CHMOD           affect file and/or directory permissions\nIf something happens and your transfer doesn’t complete, simply run the exact command again and the transfer will pick up where it left off.\n\n\nLarge transfers over 500 GiB\nFor large transfer, over 500 GiB for instance, it is better to submit the transfer as a job to the copyq queue. To read the process of how to achieve this, follow this link.",
    "crumbs": [
      "Home",
      "Tutorials",
      "File Transfer"
    ]
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Tutorials for Gadi users.\nTo learn more about Gadi visit https://opus.nci.org.au/x/4gD.",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#policies",
    "href": "tutorials.html#policies",
    "title": "Tutorials",
    "section": "Policies",
    "text": "Policies",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#accounts",
    "href": "tutorials.html#accounts",
    "title": "Tutorials",
    "section": "Accounts",
    "text": "Accounts",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#login-to-gadi",
    "href": "tutorials.html#login-to-gadi",
    "title": "Tutorials",
    "section": "Login to Gadi",
    "text": "Login to Gadi\n\nLinux tutorials\n\n\nHow to use man pages",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#file-transfer",
    "href": "tutorials.html#file-transfer",
    "title": "Tutorials",
    "section": "File Transfer",
    "text": "File Transfer",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#hpc-system-gadi-nodes-and-cores",
    "href": "tutorials.html#hpc-system-gadi-nodes-and-cores",
    "title": "Tutorials",
    "section": "HPC System: Gadi (nodes and cores)",
    "text": "HPC System: Gadi (nodes and cores)\n\nCheck your project allocation",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#user-support",
    "href": "tutorials.html#user-support",
    "title": "Tutorials",
    "section": "User Support",
    "text": "User Support",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#check-your-job-status",
    "href": "tutorials.html#check-your-job-status",
    "title": "Tutorials",
    "section": "Check your job status",
    "text": "Check your job status",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#cancel-a-job",
    "href": "tutorials.html#cancel-a-job",
    "title": "Tutorials",
    "section": "Cancel a job",
    "text": "Cancel a job",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#monitor-your-job",
    "href": "tutorials.html#monitor-your-job",
    "title": "Tutorials",
    "section": "Monitor your job",
    "text": "Monitor your job",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#job-output-and-error-files",
    "href": "tutorials.html#job-output-and-error-files",
    "title": "Tutorials",
    "section": "Job output and error files",
    "text": "Job output and error files",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#check-what-software-is-available",
    "href": "tutorials.html#check-what-software-is-available",
    "title": "Tutorials",
    "section": "Check what software is available",
    "text": "Check what software is available\n\nLicensed software",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#how-to-use-modules",
    "href": "tutorials.html#how-to-use-modules",
    "title": "Tutorials",
    "section": "How to use modules",
    "text": "How to use modules\n\nCustomed modules",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "tutorials.html#use-python-on-gadi",
    "href": "tutorials.html#use-python-on-gadi",
    "title": "Tutorials",
    "section": "Use Python on Gadi",
    "text": "Use Python on Gadi\n\nLoad Python module (example in terminal and script)\n\n\nManage Python Packages using virtual environment\n\n\nUse JupyterLab",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "dlweather/4_inference_10y_rollout.html",
    "href": "dlweather/4_inference_10y_rollout.html",
    "title": "Lucie - Inference",
    "section": "",
    "text": "!module list\n\nCurrently Loaded Modulefiles:\n 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \n\n\n\nfrom math import ceil\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n\nimport torch\n\nimport sys,os,time\nnb_dir=\"/g/data/dk92/notebooks/examples-aiml/lucie\"\nsys.path.append(f\"{nb_dir}/models\")\nfrom torch_harmonics_local import *\nfrom LUCIE_inference import inference\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n\n\n# load the ground truth for clim_bias from the original work\ndata = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\ntrue_clim = torch.tensor(np.mean(data, axis=0)).to(device).permute(2,0,1)\n\n\n# load the normalization scalars from the original work\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")     # standardized data with mean and stds generated from dataset_generator.py\ndata_inp = torch.tensor(data[\"data_inp\"],dtype=torch.float32)     # input data \ndata_tar = torch.tensor(data[\"data_tar\"],dtype=torch.float32)\nraw_means = torch.tensor(data[\"raw_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nraw_stds = torch.tensor(data[\"raw_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nprog_means = raw_means[:,:5]\nprog_stds = raw_stds[:,:5]\ndiag_means = torch.tensor(data[\"diag_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiag_stds = torch.tensor(data[\"diag_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_means = torch.tensor(data[\"diff_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_stds = torch.tensor(data[\"diff_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n\n\n# initialize the SFNO model\ngrid='legendre-gauss'\nnlat = 48\nnlon = 96\nhard_thresholding_fraction = 0.9\nlmax = ceil(nlat / 1)\nmmax = lmax\nmodes_lat = int(nlat * hard_thresholding_fraction)\nmodes_lon = int(nlon//2 * hard_thresholding_fraction)\nmodes_lat = modes_lon = min(modes_lat, modes_lon)\nsht = RealSHT(nlat, nlon, lmax=modes_lat, mmax=modes_lon, grid=grid, csphase=False)\nradius=6.37122E6\ncost, quad_weights = legendre_gauss_weights(nlat, -1, 1)\nquad_weights = (torch.as_tensor(quad_weights).reshape(-1, 1)).to(device)\nmodel = SphericalFourierNeuralOperatorNet(params = {}, spectral_transform='sht', filter_type = \"linear\", operator_type='dhconv', img_shape=(48, 96),\n                    num_layers=8, in_chans=7, out_chans=6, scale_factor=1, embed_dim=72, activation_function=\"silu\", big_skip=True, pos_embed=\"latlon\", use_mlp=True,\n                                            normalization_layer=\"instance_norm\", hard_thresholding_fraction=hard_thresholding_fraction,\n                                            mlp_ratio = 2.).to(device)\ngadi=False\n\nif gadi:\n  #load checkpoint trained on Gadi\n  print(\"loading LUCIE checkpoint trained on Gadi\")\n  pth = torch.load(f'{nb_dir}/checkpoints/nci_rep_lucie_340.pt')\n  model.load_state_dict(pth[\"model_state_dict\"])\nelse:\n  print(\"loading original LUCIE checkpoint\")\n  pth = torch.load(f'{nb_dir}/checkpoints/regular_8x72_fftreg_baseline.pth')\n  model.load_state_dict(pth)\n    \n# run rollout for 10 years\nforcing = data_inp[:1460,-2:]   # repeating tisr and constant oro\nrollout_step = 14600 # 10y of rollout\ninitial_frame_idx = 16000+100\nforcing_initial_idx = (16000+100) % 1460 + 1\nrollout = inference(model, rollout_step, data_inp[initial_frame_idx].unsqueeze(0).to(device), forcing.to(device), forcing_initial_idx, prog_means, prog_stds, diag_means, diag_stds, diff_stds)\nprint(rollout.shape)\n\nloading original LUCIE checkpoint\n\n\n/g/data/dk92/notebooks/examples-aiml/lucie/models/torch_harmonics_local.py:1242: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(enabled=False):\n/g/data/dk92/notebooks/examples-aiml/lucie/models/torch_harmonics_local.py:1263: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(enabled=False):\n\n\n(14600, 6, 48, 96)\n\n\n\nVisualize Rollout Results\n\n# prepare for visualization\ndef generate_t30_grid():\n    # T62 Gaussian grid parameters\n    nlat = 48  # Number of latitudes\n    nlon = 96  # Number of longitudes\n\n    # Gaussian latitudes and weights\n    latitudes, weights = np.polynomial.legendre.leggauss(nlat)\n    latitudes = np.arcsin(latitudes) * (180.0 / np.pi)  # Convert to degrees\n\n    # Longitudes\n    longitudes = np.linspace(0, 360, nlon, endpoint=False)\n\n    return latitudes, longitudes\n\nlat, lon = generate_t30_grid()\n\n\nvars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'precipitation']\nnvars=len(vars)\nLon, Lat = np.meshgrid(lon, lat)\n\n\n\nCheck the Spatial Pattern\n\n#results from Yue's checkpoint\n# path = torch.load(f'{wdir}/checkpoints/136618981.gadi-pbs/regular_training_checkpoint.pth')\n# model.load_state_dict(path)\n\n# better checkpoint\n#path = torch.load(f'{wdir}/checkpoints/137286020.gadi-pbs/lucie_205.pt')\n#path = torch.load(f'{wdir}/checkpoints/137478626.gadi-pbs/lucie_158.pt')\n#path = torch.load(f'{wdir}/checkpoints/138987659.gadi-pbs/lucie_340.pt')\n\n#model.load_state_dict(path[\"model_state_dict\"])\n\n#rollout = inference(model, rollout_step, data_inp[initial_frame_idx].unsqueeze(0).to(device), forcing.to(device), forcing_initial_idx, prog_means, prog_stds, diag_means, diag_stds, diff_stds)\n\n\n# visualize the final timestep of each ouptut var\nw,h = 12,5\nfig,axs = plt.subplots(nvars,1, figsize=(w,h*nvars),subplot_kw={'projection': ccrs.PlateCarree()},squeeze=False)\nLon, Lat = np.meshgrid(lon, lat)\nfor ii in range(nvars):\n    pcm = axs[ii,0].pcolormesh(Lon,Lat,rollout[-1,ii,:,:])\n    axs[ii,0].coastlines()\n    axs[ii,0].set_title(f\"{vars[ii]} at step {rollout.shape[0]}\")\n    fig.colorbar(pcm, ax=axs[ii])\n\n\n\n\n\n\n\n\n\n\nCheck Climatology Bias\n\n# mean clim_bias between year 1 and 10, it is expected to be different from what reported in the training \nrollout_clim = torch.mean(torch.tensor(rollout[1460:]).to(device),dim=0)\nclim_bias = torch.mean(torch.abs(rollout_clim - true_clim))\nclim_bias\n\ntensor(18.2533, device='cuda:0')\n\n\n\n# relative clim_bias by vars\nrel_clim_bias = torch.mean(torch.abs((rollout_clim - true_clim)/true_clim),dim=(-2,-1))\nlist(zip(vars,rel_clim_bias.tolist()))\n\n[('temperature', 0.0021463886369019747),\n ('humidity', 0.09310100972652435),\n ('u_wind', 2.2337892055511475),\n ('v_wind', 2.5637850761413574),\n ('surface_pressure', 0.001107409130781889),\n ('precipitation', 0.14571672677993774)]\n\n\n\n\nCompare Global Mean over Time\n\n# Compare global mean over time [year 1, year 10]\ndata = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\ntrue_clim_t = np.mean(data[1460:14600], axis=(1,2))\npred = torch.tensor(rollout[1460:]).detach()\npred_clim_t = torch.mean(pred,dim=(-1,-2))\n\n\nfig,axs = plt.subplots(nvars,1, figsize=(w*3,h*nvars))\nfor ii in range(nvars):\n    pcm1 = axs[ii].plot(true_clim_t[:,ii],label=f\"targ_{vars[ii]}\")\n    pcm2 = axs[ii].plot(pred_clim_t[:,ii],label=f\"pred_{vars[ii]}\")\n    axs[ii].set_title(f\"{vars[ii]}\")\n    axs[ii].legend(loc='upper left', bbox_to_anchor=(1.003, 1),fontsize=16)\n\n\n\n\n\n\n\n\n\n\nSuggested Exercises\n\nCompare rollout results from both checkpoints.\nUse a different metric to quantify the difference between prediction and the ground truth over time.\nTrack the spatial pattern evolution of an individual variable field over time\nRun the notebook /g/data/dk92/notebooks/examples-aiml/lucie/modified_training.ipynb and generate the 10-year inference for its model with the checkpoints nci_mod_lucie_193.pt and/or nci_mod_lucie_219.pt inside the directory /g/data/dk92/notebooks/examples-aiml/lucie/checkpoints/\n\n\n\nThe END",
    "crumbs": [
      "Home",
      "Dlweather",
      "Lucie - Inference"
    ]
  },
  {
    "objectID": "dlweather/1_data_inspection.html",
    "href": "dlweather/1_data_inspection.html",
    "title": "Lucie - Data Inspection",
    "section": "",
    "text": "!module list\n\nCurrently Loaded Modulefiles:\n 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \n\n\n\nimport scipy.stats as stats\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\n\nnb_dir = \"/g/data/dk92/notebooks/examples-aiml/lucie\"\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")\ndata.files\n\n['temperature',\n 'humidity',\n 'u_wind',\n 'v_wind',\n 'surface_pressure',\n 'precipitation',\n 'tisr',\n 'orography']\n\n\n\nNormalization Scalars\n\n# comparing mean, std to era5, t-&gt; t1000, q -&gt; q1000, z-&gt;z200, u-&gt; u200, v-&gt; v200\nfor var in data.files:\n    mm = data[var].mean()\n    ss = data[var].std()\n    print(f\"{var}: mean={mm}, std={ss}\")\n\ntemperature: mean=277.4626770019531, std=17.816375732421875\nhumidity: mean=0.006629979237914085, std=0.005532822106033564\nu_wind: mean=10.695755004882812, std=15.56001091003418\nv_wind: mean=-0.02342492900788784, std=12.149330139160156\nsurface_pressure: mean=96811.390625, std=9123.89453125\nprecipitation: mean=0.0006106931250542402, std=0.001604488817974925\ntisr: mean=1079093.125, std=1444145.25\norography: mean=367.17120361328125, std=809.2915649414062\n\n\n\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")\ndata_inp = data[\"data_inp\"]     # input data \ndata_tar = data[\"data_tar\"]\nraw_means = data[\"raw_means\"]\nraw_stds = data[\"raw_stds\"]\nprog_means = raw_means[:5] # this is literally zero?\nprog_stds = raw_stds[:5] # this is literally zero?\ndiag_means = data[\"diag_means\"]\ndiag_stds = data[\"diag_stds\"]\ndiff_means = data[\"diff_means\"]\ndiff_stds = data[\"diff_stds\"]\n\n\nvars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'tisr', 'orography']\nlist(zip(vars, raw_means, raw_stds))\n\n[('temperature', np.float64(277.462646484375), np.float64(17.816408157348633)),\n ('humidity',\n  np.float64(0.006629981566220522),\n  np.float64(0.005532818380743265)),\n ('u_wind', np.float64(10.695756912231445), np.float64(15.56002426147461)),\n ('v_wind', np.float64(-0.023422522470355034), np.float64(12.14930534362793)),\n ('surface_pressure', np.float64(96811.390625), np.float64(9123.8916015625)),\n ('tisr', np.float64(1079094.75), np.float64(1444147.125)),\n ('orography', np.float64(367.17120361328125), np.float64(809.2916259765625))]\n\n\n\nvars= ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure']\nlist(zip(vars, diff_means, diff_stds))\n\n[('temperature',\n  np.float64(9.803541615838185e-05),\n  np.float64(1.4138480424880981)),\n ('humidity',\n  np.float64(2.0586108817610693e-08),\n  np.float64(0.0005922476993873715)),\n ('u_wind',\n  np.float64(-3.1180163205135614e-05),\n  np.float64(4.642238140106201)),\n ('v_wind', np.float64(-5.063314802100649e-06), np.float64(5.960874080657959)),\n ('surface_pressure',\n  np.float64(0.0021911216899752617),\n  np.float64(241.07797241210938))]\n\n\n\nlist(zip(vars,[ diff_stds[ii] / raw_stds[ii] for ii in range(5) ]))\n\n[('temperature', np.float64(0.07935651395059314)),\n ('humidity', np.float64(0.1070426785467356)),\n ('u_wind', np.float64(0.298343888293286)),\n ('v_wind', np.float64(0.4906349714705557)),\n ('surface_pressure', np.float64(0.026422713348635562))]\n\n\n\n# for log scale preciptation\ndiag_means,diag_stds\n\n(array([0.05197881]), array([0.11017133]))\n\n\n\n\nVariable Fields\n\nwdir = \"/g/data/z00/yxs900/neuraloperators/sfno/curriculum_learning/lowRes/experiments/03_LUCIE/LUCIE_fix\"\nraw_data = np.load(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")\n\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")\ndata_inp = data[\"data_inp\"]     # input data \ndata_tar = data[\"data_tar\"]\n\ninp_vars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'tisr', 'orography']\ntar_vars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure','precipitation']\nvars_to_plot = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'tisr', 'orography','precipitation']\n\nraw_data[\"temperature\"].shape, data_inp.shape, data_tar.shape\n\n((16538, 48, 96), (16537, 7, 48, 96), (16537, 6, 48, 96))\n\n\n\ndef img2spectrum(img,ftr):\n    npix = img.shape[-2], img.shape[-1]\n    fft_img = np.fft.fftn(img)\n    fft_amp = np.abs(fft_img)**2\n    fft_amp = fft_amp.flatten()\n\n    kfreq_x = np.fft.fftfreq(npix[1]) * npix[1] # wave vector\n    kfreq_y = np.fft.fftfreq(npix[0]) * npix[0] # wave vector\n    kfreq2D = np.meshgrid(kfreq_x, kfreq_y)\n    knrm = np.sqrt(kfreq2D[0]**2 + kfreq2D[1]**2)\n    knrm = knrm.flatten()\n\n    #kbins = np.arange(0.5, min(*npix)//2+1, 1.)\n    kbins = np.arange(0.5, ftr, 1.)\n    kvals = 0.5 * (kbins[1:] + kbins[:-1])\n    Abins, _, _ = stats.binned_statistic(\n        knrm,\n        fft_amp,\n        statistic='mean',\n        bins=kbins\n    )\n\n    Abins *= np.pi * (kbins[1:]**2 - kbins[:-1]**2)\n    return kvals, Abins\n\n\n# plot the spatial and spectral pattern of all variable fields\nfig, axes = plt.subplots(len(vars_to_plot), 3, figsize=(16, 3 * len(vars_to_plot)))\nif len(vars_to_plot) == 1:\n    axes = [axes] \n\nfor ii, var in enumerate(vars_to_plot):\n    #print(var)\n    \n    # First column: original \n    if var in inp_vars:\n        idx = inp_vars.index(var)\n        data_norm0 = data_inp[:,idx,:,:].mean(axis=(0,))\n        axes[ii, 0].pcolormesh(data_norm0)\n        axes[ii, 0].set_title(f\"normalized input {var}\")\n\n        ftr = max(*data_norm0.shape)\n        kvals0, Abins0 = img2spectrum(data_norm0,ftr)\n        axes[ii, 2].plot(kvals0, Abins0/max(Abins0), label=\"normalized input\")\n\n    # Second column: normalized input data\n    if var in tar_vars:\n        idx = tar_vars.index(var)\n        data_norm1 = data_tar[:,idx,:,:].mean(axis=(0,))\n        axes[ii, 1].pcolormesh(data_norm1)\n        axes[ii, 1].set_title(f\"normalized tar {var}\")\n\n        ftr = max(*data_norm1.shape)\n        kvals1, Abins1 = img2spectrum(data_norm1,ftr)\n        axes[ii, 2].plot(kvals1, Abins1/max(Abins1), label=\"normalized tar\")\n        \n    # third column: normalized output data\n    axes[ii, 2].legend()\n    axes[ii, 2].set_title(f\"normalized spectrum: {var}\")\n    \nplt.subplots_adjust(hspace=0.5)\n\n\n\n\n\n\n\n\n\n# plot the global mean over time of each variable\n\nfig, axes = plt.subplots(len(vars_to_plot), 3, figsize=(16, 3 * len(vars_to_plot)))\nif len(vars_to_plot) == 1:\n    axes = [axes] \n\nfor ii, var in enumerate(vars_to_plot):\n    #print(var)\n    \n    # First column: normalized input data\n    data = raw_data[var]\n    axes[ii, 0].plot(data.mean(axis=(-2,-1)))\n    axes[ii, 0].set_title(f\"original {var}\")\n\n    # Second column: normalized output data\n    if var in inp_vars:\n        idx = inp_vars.index(var)\n        data_norm0 = data_inp[:,idx,:,:].mean(axis=(-2,-1))\n        axes[ii, 1].plot(data_norm0)\n        axes[ii, 1].set_title(f\"normalized input {var}\")\n\n    # third column: normalized input vs output data spectrum\n    if var in tar_vars:\n        idx = tar_vars.index(var)\n        data_norm1 = data_tar[:,idx,:,:].mean(axis=(-2,-1))\n        axes[ii, 2].plot(data_norm1)\n        axes[ii, 2].set_title(f\"normalized tar {var}\")\n    \nplt.subplots_adjust(hspace=0.5)\n\n\n\n\n\n\n\n\n\n# plot the distribution of all variable fileds\n\nwdir = \"/g/data/z00/yxs900/neuraloperators/sfno/curriculum_learning/lowRes/experiments/03_LUCIE/LUCIE_fix\"\nraw_data = np.load(f\"{wdir}/era5_T30_regridded.npz\")\n\ndata = np.load(f\"{wdir}/era5_T30_preprocessed.npz\")\ndata_inp = data[\"data_inp\"]     # input data \ndata_tar = data[\"data_tar\"]\n\ninp_vars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'tisr', 'orography']\ntar_vars = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure','precipitation']\nvars_to_plot = ['temperature', 'humidity', 'u_wind', 'v_wind', 'surface_pressure', 'tisr', 'orography','precipitation']\n\nfig, axes = plt.subplots(len(vars_to_plot), 3, figsize=(16, 3 * len(vars_to_plot)))\nif len(vars_to_plot) == 1:\n    axes = [axes] \n\nfor ii, var in enumerate(vars_to_plot):\n    #print(var)\n    data = raw_data[var]\n    data = data[~np.isnan(data)]\n    # Compute min, max, 5th and 95th percentiles\n    vmin = data.min()\n    vmax = data.max()\n\n    # First column: original \n    axes[ii, 0].hist(data, bins=100,density=True)\n    axes[ii, 0].set_title(f\"original {var}: min={vmin:.2f}, max={vmax:.2f}\")\n\n    # Second column: normalized input data\n    if var in inp_vars:\n        idx = inp_vars.index(var)\n        data_norm0 = data_inp[:,idx,:,:].flatten()\n        axes[ii, 1].hist(data_norm0, bins=100, density=True)\n        axes[ii, 1].set_title(f\"normalized input {var}\")\n\n    # third column: normalized output data\n    if var in tar_vars:\n        idx = tar_vars.index(var)\n        data_norm1 = data_tar[:,idx,:,:].flatten()\n        axes[ii, 2].hist(data_norm1, bins=100, density=True)\n        axes[ii, 2].set_title(f\"normalized tar {var}\")\n    \nplt.subplots_adjust(hspace=0.5)\n\n\n\n\n\n\n\n\n\n\nThe END",
    "crumbs": [
      "Home",
      "Dlweather",
      "Lucie - Data Inspection"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCI Learning Hub",
    "section": "",
    "text": "Workshops, tutorials, and learning resources for Australian researchers.\nTo learn more about Gadi visit https://opus.nci.org.au/x/4gD."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Self-paced Courses",
    "section": "",
    "text": "E-research community and NCI training courses are available here.\n\nProgramming Basics\nVersion Control with Git\nIntroduction to Python\nIntroduction to the Unix Shell\nC Programming for Scientific Computing\n\n\nDeep Learning\nIntroduction to Neural Networks and PyTorch\nDeep Learning Model Development in Weather and Climate Studies\n* Lucie - Data Inspection Notebook\n* Lucie - Replicate Training Notebook\n* Lucie - Modified Training Notebook\n* Lucie - Inference Notebook\n\n\nHigh Performance Computing\nHands-on with Gadi\nHPC 101 Series\n\n\nParallel Programming\nIntroduction to Parallel Programming using Python\nParallel Python Introduction to Dask\nParallel Python Introduction to Numba\nParallel Python Introduction to CuPy\nGPU Programming Introduction to CUDA\nParallel Programming Introduction to MPI\n\n\nGenAI/LLM",
    "crumbs": [
      "Home",
      "Self-paced Courses"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "dlweather/3_modified_training.html",
    "href": "dlweather/3_modified_training.html",
    "title": "Lucie - Modified Training",
    "section": "",
    "text": "!module list\n\nCurrently Loaded Modulefiles:\n 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \n\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport sys,os,time\nnb_dir=\"/g/data/dk92/notebooks/examples-aiml/lucie\"\nsys.path.append(f\"{nb_dir}/models\")\nfrom torch_harmonics_local_v2 import *\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n\n\n# normalization function used for static channels. \ndef _minmax(img):\n        return torch.as_tensor((img-img.min())/(img.max()-img.min()))\n\ndef generate_t30_grid():\n    nlat = 48  # Number of latitudes\n    nlon = 96  # Number of longitudes\n\n    # Gaussian latitudes and weights\n    latitudes, weights = np.polynomial.legendre.leggauss(nlat)\n    latitudes = np.arcsin(latitudes) * (180.0 / np.pi)  # Convert to degrees\n\n    # Longitudes\n    longitudes = np.linspace(0, 360, nlon, endpoint=False)\n\n    return latitudes, longitudes\n\n\n# modified inference function adapted to lat,lon inputs\ndef inference(model, steps, initial_frame, forcing, initial_forcing_idx, prog_means, prog_stds, diag_means, diag_stds, diff_stds):\n    inf_data = []\n    inp_const =const_chans.to(device,dtype=torch.float32)\n    model.eval()\n    with torch.no_grad():\n        inp_val = initial_frame\n        for i in range(steps):\n            forcing_idx = (initial_forcing_idx + i) % 1460      # tisr is repeating and orography is \n            previous = inp_val[:,:5,:,:]\n            inpc_val = torch.cat([inp_const, inp_val],dim=1)\n            pred = model(inpc_val)\n            pred[:,:5,:,:] = pred[:,:5,:,:] * diff_stds         # denormalize the predicted tendency\n            \n            # demornalzie the previous time step and add to the tendecy to reconstruct the current field\n            pred[:,:5,:,:] += previous[:,:5,:,:] * prog_stds + prog_means\n            \n            tp_frame = pred[:,5:,:,:] * diag_stds + diag_means\n            raw = torch.cat((pred[:,:5,:,:],tp_frame), 1)\n            \n            inp_val = (raw[:,:5,:,:] - prog_means) / prog_stds      # normalize the current time step for autoregressive prediction\n            inp_val = torch.cat((inp_val, forcing[forcing_idx,:,:,:].reshape(1,2,48,96)), dim=1)\n            raw = raw.cpu().clone().detach().numpy()\n            inf_data.append(raw[0])\n\n    inf_data = np.array(inf_data)\n    inf_data[:,5,:,:] = (np.exp(inf_data[:,5,:,:]) - 1) * 1e-2      # denormalzie precipitation that was normalized in log space\n    return inf_data\n\n\n# define util functions, overide the one defined in LUCIE_train.py\ndef integrate_grid(ugrid):\n    dlon = 2 * torch.pi / nlon\n    out = torch.sum(ugrid * quad_weights * dlon, dim=(-2, -1))\n    return out\n\ndef l2loss_sphere(prd, tar, relative=False, squared=True):\n    loss = integrate_grid((prd - tar)**2).sum(dim=-1)\n    if relative:\n        loss = loss / integrate_grid(tar**2).sum(dim=-1)\n\n    if not squared:\n        loss = torch.sqrt(loss)\n    loss = loss.mean()\n\n    return loss\n\ndef train_model(model, tdl, optimizer, scheduler=None, nepochs=20, loss_fn='l2'):\n    infer_bias = 1e+80\n    ibs = torch.zeros(1,nepochs)\n    best_bias = 1e+80\n    recall_count = 0\n    acc_losses = []\n    epoch_times = []\n    #ckpt_dir=f\"{os.environ['PBS_O_WORKDIR']}/checkpoints/{os.environ['PBS_JOBID']}\"\n    for epoch in range(nepochs):\n        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n        print(f'--------------------------------------------------------------------------------')\n        print(f\"{tstamp}: epoch {epoch} start\")\n        epoch_start = time.time()\n        \n        if epoch &lt; 149:\n            if scheduler is not None:\n                scheduler.step()\n                print(f'using scheduler: current learning rate = {scheduler.get_last_lr()}')\n        else:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = 1e-6\n                \n            print(f\"current learning rate = {optimizer.param_groups[0]['lr']}\")\n        \n        optimizer.zero_grad()\n\n        acc_loss = 0\n        model.train()\n        #batch_num = 0\n        for inp, tar in tdl:\n            #batch_num += 1\n            #loss = 0\n\n            #inp = inp.to(device)\n            # adding lat, lon as the first two channels\n            inpc = torch.cat([const_chans_t, inp],dim=1).to(device,dtype=torch.float32)\n            tar = tar.to(device)\n            prd = model(inpc)\n\n            # prd and tar shape are not affected, keep this section\n            loss_delta = l2loss_sphere(prd[:,:5,:,:], tar[:,:5,:,:], relative=True)\n            loss_tp = torch.mean((prd[:,5:,:,:]-tar[:,5:,:,:])**2)\n            loss = loss_delta + loss_tp / tar.shape[1]\n\n            if epoch &gt; 150:\n                #print(f\"add spectral loss\")\n                lat_index = np.r_[7:15, 32:40]\n                out_fft = torch.mean(torch.abs(torch.fft.rfft(prd[:,:,lat_index,:],dim=3)),dim=2)\n                target_fft = torch.mean(torch.abs(torch.fft.rfft(tar[:,:,lat_index,:],dim=3)),dim=2)\n                loss_reg = 0.05 * torch.mean(torch.abs(out_fft - target_fft))\n                loss = loss + loss_reg\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            acc_loss += loss.item()* inp.size(0)\n            \n        acc_losses.append(acc_loss / len(tdl.dataset))\n\n        epoch_times.append(time.time() - epoch_start)\n        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n        print(f'{tstamp}: Epoch {epoch} summary:')\n        print(f'time taken: {epoch_times[-1]}')\n        print(f'nsamples / sec: {len(tdl.dataset)/epoch_times[-1]}')\n        print(f'average training loss: {acc_losses[-1]}')\n\n        if epoch &gt;= 60:\n            rollout_steps = 2920\n            rollout = torch.tensor(inference(model, rollout_steps, data_inp[0:1].to(device), data_inp[:1460,-2:].to(device), 1, prog_means, prog_stds, diag_means, diag_stds, diff_stds)).to(device)\n            rollout_clim = torch.mean(rollout[1460:],dim=0)\n            clim_bias = torch.mean(torch.abs(rollout_clim - true_clim))\n            ibs[0,epoch] = clim_bias\n            if len(ibs&gt;0)&lt;=20:\n                infer_bias = torch.mean(torch.tensor(ibs[0,60:epoch+1]))\n            else:\n                infer_bias = torch.mean(ibs[0,epoch-20:epoch+1])\n\n            print(f'clim_bias: {clim_bias}')\n            print(f'infer_bias: {infer_bias}')\n            if clim_bias &lt;= best_bias:\n                print(f\"new best clim_bias, save checkpoint\")\n                best_bias = clim_bias\n                #torch.save({\"epoch\":epoch,\"model_state_dict\":model.state_dict(),\"optim_state_dict\":optimizer.state_dict(),\"sch_state_dict\":scheduler.state_dict()},f\"{ckpt_dir}/lucie_{epoch}.pt\")\n                torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n         \n            if epoch % 10 == 0:\n                if ~torch.isnan(clim_bias): \n                    if clim_bias &lt;= infer_bias:\n                        #print(f\"clim_bias &lt;= {infer_bias}, save checkpoint\")\n                        #infer_bias = clim_bias\n                        #torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n                        recall_count = 0\n                    else:\n                        print(f\"clim_bias &gt; {infer_bias}, recall from latest checkpoint\")\n                        state_pth = torch.load(f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n                        model.load_state_dict(state_pth)\n                        recall_count += 1\n                        if recall_count &gt; 3:\n                            break\n\n\n# load data\ndata = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\ntrue_clim = torch.tensor(np.mean(data, axis=0)).to(device).permute(2,0,1)\n\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")     # standardized data with mean and stds generated from dataset_generator.py\ndata_inp = torch.tensor(data[\"data_inp\"],dtype=torch.float32)     # input data \ndata_tar = torch.tensor(data[\"data_tar\"],dtype=torch.float32)\nraw_means = torch.tensor(data[\"raw_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nraw_stds = torch.tensor(data[\"raw_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nprog_means = raw_means[:,:5]\nprog_stds = raw_stds[:,:5]\ndiag_means = torch.tensor(data[\"diag_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiag_stds = torch.tensor(data[\"diag_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_means = torch.tensor(data[\"diff_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_stds = torch.tensor(data[\"diff_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n\nntrain = 16000\ntrain_set = TensorDataset(data_inp[:ntrain],data_tar[:ntrain])\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n\n# generate the const channels\nlats, lons = generate_t30_grid()\nlon2d, lat2d = np.meshgrid(lons, lats)\nconst_chans = _minmax(np.stack([_minmax(lat2d), _minmax(lon2d)])).unsqueeze(0)\nconst_chans_t = const_chans.expand(16, 2, 48, 96)\n\n\n# set the model\nnlat = 48\nnlon = 96\nhard_thresholding_fraction = 0.9\ncost, quad_weights = legendre_gauss_weights(nlat, -1, 1)\nquad_weights = (torch.as_tensor(quad_weights).reshape(-1, 1)).to(device)\nmodel = SphericalFourierNeuralOperatorNet(params = {}, spectral_transform='sht', filter_type = \"linear\", operator_type='dhconv', img_shape=(48, 96),num_layers=8, in_chans=9, out_chans=6, scale_factor=1, embed_dim=72, activation_function=\"silu\", big_skip=True, pos_embed=False, use_mlp=True,normalization_layer=\"instance_norm\", hard_thresholding_fraction=hard_thresholding_fraction,mlp_ratio = 2.).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\nscheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-5)\nprint(sum(p.numel() for p in model.parameters()))\n\n3791376\n\n\n\n# it takes roughly 75 sec per epoch to train the modified LUCIE model on a single V100\n# checkpoints saved at epoch 193 and 219 are available in `/g/data/dk92/notebooks/examples-aiml/lucie/checkpoints/`\nnepochs=3\ntrain_model(model, train_loader, optimizer, scheduler=scheduler, nepochs=nepochs)\n\n--------------------------------------------------------------------------------\n17:55:28: epoch 0 start\nusing scheduler: current learning rate = [9.999013075636805e-05]\n\n\n/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n\n\n17:56:44: Epoch 0 summary:\ntime taken: 75.56519341468811\nnsamples / sec: 211.73769664288284\naverage training loss: 0.5649356337823864\n--------------------------------------------------------------------------------\n17:56:44: epoch 1 start\nusing scheduler: current learning rate = [9.996052735444863e-05]\n17:57:58: Epoch 1 summary:\ntime taken: 74.5183458328247\nnsamples / sec: 214.71222718623653\naverage training loss: 0.4445175377408696\n--------------------------------------------------------------------------------\n17:57:58: epoch 2 start\nusing scheduler: current learning rate = [9.991120277927221e-05]\n17:59:13: Epoch 2 summary:\ntime taken: 74.4180178642273\nnsamples / sec: 215.00169527749802\naverage training loss: 0.41059033054741506\n\n\n\nSuggested Exercises\n\nidentify the difference between the above model and the model used in /g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb.\ncompare learning curve of this training and the one defined in /g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb.\ncompare rollouts generated from this model and the one trained in /g/data/dk92/notebooks/examples-aiml/lucie/replicate_training.ipynb.\nmodify the model further to have higher embedding dimension in the feature space.\n\n\n\nThe END",
    "crumbs": [
      "Home",
      "Dlweather",
      "Lucie - Modified Training"
    ]
  },
  {
    "objectID": "dlweather/2_replicate_training.html",
    "href": "dlweather/2_replicate_training.html",
    "title": "Lucie - Replicate Training",
    "section": "",
    "text": "!module list\n\nCurrently Loaded Modulefiles:\n 1) openmpi/4.1.5   2) singularity   3) NCI-ai-ml/25.07   4) pbs  \n\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport sys,os,time\nnb_dir=\"/g/data/dk92/notebooks/examples-aiml/lucie\"\nsys.path.append(f\"{nb_dir}/models\")\nfrom torch_harmonics_local import *\nfrom LUCIE_inference import inference\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n\n\n# define util functions, overide the one defined in LUCIE_train.py\ndef integrate_grid(ugrid):\n    dlon = 2 * torch.pi / nlon\n    out = torch.sum(ugrid * quad_weights * dlon, dim=(-2, -1))\n    return out\n\ndef l2loss_sphere(prd, tar, relative=False, squared=True):\n    loss = integrate_grid((prd - tar)**2).sum(dim=-1)\n    if relative:\n        loss = loss / integrate_grid(tar**2).sum(dim=-1)\n\n    if not squared:\n        loss = torch.sqrt(loss)\n    loss = loss.mean()\n\n    return loss\n\ndef train_model(model, tdl, optimizer, scheduler=None, nepochs=20, loss_fn='l2'):\n    infer_bias = 1e+80\n    ibs = torch.zeros(1,nepochs)\n    best_bias = 1e+80\n    recall_count = 0\n    acc_losses = []\n    epoch_times = []\n    #ckpt_dir=f\"{os.environ['PBS_O_WORKDIR']}/checkpoints/{os.environ['PBS_JOBID']}\"\n    ckpt_dir=os.getcwd()\n    for epoch in range(nepochs):\n        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n        print(f'--------------------------------------------------------------------------------')\n        print(f\"{tstamp}: epoch {epoch} start\")\n        epoch_start = time.time()\n\n        if epoch &lt; 149:\n            if scheduler is not None:\n                scheduler.step()\n                print(f'using scheduler: current learning rate = {scheduler.get_last_lr()}')\n        else:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = 1e-6\n\n            print(f\"current learning rate = {optimizer.param_groups[0]['lr']}\")\n\n        optimizer.zero_grad()\n\n        acc_loss = 0\n        model.train()\n        #batch_num = 0\n        for inp, tar in tdl:\n            #batch_num += 1\n            #loss = 0\n\n            inp = inp.to(device)\n            tar = tar.to(device)\n            prd = model(inp)\n\n            loss_delta = l2loss_sphere(prd[:,:5,:,:], tar[:,:5,:,:], relative=True)\n            loss_tp = torch.mean((prd[:,5:,:,:]-tar[:,5:,:,:])**2)\n            loss = loss_delta + loss_tp / tar.shape[1]\n\n            if epoch &gt; 150:\n                #print(f\"add spectral loss\")\n                lat_index = np.r_[7:15, 32:40]\n                out_fft = torch.mean(torch.abs(torch.fft.rfft(prd[:,:,lat_index,:],dim=3)),dim=2)\n                target_fft = torch.mean(torch.abs(torch.fft.rfft(tar[:,:,lat_index,:],dim=3)),dim=2)\n                loss_reg = 0.05 * torch.mean(torch.abs(out_fft - target_fft))\n                loss = loss + loss_reg\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            acc_loss += loss.item()* inp.size(0)\n\n        acc_losses.append(acc_loss / len(tdl.dataset))\n\n        epoch_times.append(time.time() - epoch_start)\n        tstamp=time.strftime(\"%H:%M:%S\",time.localtime())\n        print(f'{tstamp}: Epoch {epoch} summary:')\n        print(f'time taken: {epoch_times[-1]}')\n        print(f'nsamples / sec: {len(tdl.dataset)/epoch_times[-1]}')\n        print(f'average training loss: {acc_losses[-1]}')\n\n        if epoch &gt;= 60:\n            rollout_steps = 2920\n            rollout = torch.tensor(inference(model, rollout_steps, data_inp[0:1].to(device), data_inp[:1460,-2:].to(device), 1, prog_means, prog_stds, diag_means, diag_stds, diff_stds)).to(device)\n            rollout_clim = torch.mean(rollout[1460:],dim=0)\n            clim_bias = torch.mean(torch.abs(rollout_clim - true_clim))\n            ibs[0,epoch] = clim_bias\n            if len(ibs&gt;0)&lt;=20:\n                infer_bias = torch.mean(torch.tensor(ibs[0,60:epoch+1]))\n            else:\n                infer_bias = torch.mean(ibs[0,epoch-20:epoch+1])\n\n            print(f'clim_bias: {clim_bias}')\n            print(f'infer_bias: {infer_bias}')\n            if clim_bias &lt;= best_bias:\n                print(f\"new best clim_bias, save checkpoint\")\n                best_bias = clim_bias\n                #torch.save({\"epoch\":epoch,\"model_state_dict\":model.state_dict(),\"optim_state_dict\":optimizer.state_dict(),\"sch_state_dict\":scheduler.state_dict()},f\"{ckpt_dir}/lucie_{epoch}.pt\")\n                torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n\n            if epoch % 10 == 0:\n                if ~torch.isnan(clim_bias):\n                    if clim_bias &lt;= infer_bias:\n                        #print(f\"clim_bias &lt;= {infer_bias}, save checkpoint\")\n                        #infer_bias = clim_bias\n                        #torch.save(model.state_dict(), f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n                        recall_count = 0\n                    else:\n                        print(f\"clim_bias &gt; {infer_bias}, recall from latest checkpoint\")\n                        state_pth = torch.load(f\"{ckpt_dir}/regular_training_checkpoint.pth\")\n                        model.load_state_dict(state_pth)\n                        recall_count += 1\n                        if recall_count &gt; 3:\n                            break\n\n\n# load data\ndata = load_data(f\"{nb_dir}/datasets/era5_T30_regridded.npz\")[...,:6]\ntrue_clim = torch.tensor(np.mean(data, axis=0)).to(device).permute(2,0,1)\n\ndata = np.load(f\"{nb_dir}/datasets/era5_T30_preprocessed.npz\")     # standardized data with mean and stds generated from dataset_generator.py\ndata_inp = torch.tensor(data[\"data_inp\"],dtype=torch.float32)     # input data \ndata_tar = torch.tensor(data[\"data_tar\"],dtype=torch.float32)\nraw_means = torch.tensor(data[\"raw_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nraw_stds = torch.tensor(data[\"raw_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\nprog_means = raw_means[:,:5]\nprog_stds = raw_stds[:,:5]\ndiag_means = torch.tensor(data[\"diag_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiag_stds = torch.tensor(data[\"diag_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_means = torch.tensor(data[\"diff_means\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\ndiff_stds = torch.tensor(data[\"diff_stds\"],dtype=torch.float32).reshape(1,-1,1,1).to(device)\n\nntrain = 16000\ntrain_set = TensorDataset(data_inp[:ntrain],data_tar[:ntrain])\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n\n\n# set the model\nnlat = 48\nnlon = 96\nhard_thresholding_fraction = 0.9\ncost, quad_weights = legendre_gauss_weights(nlat, -1, 1)\nquad_weights = (torch.as_tensor(quad_weights).reshape(-1, 1)).to(device)\nmodel = SphericalFourierNeuralOperatorNet(params = {}, spectral_transform='sht', filter_type = \"linear\", operator_type='dhconv', img_shape=(48, 96),\n                    num_layers=8, in_chans=7, out_chans=6, scale_factor=1, embed_dim=72, activation_function=\"silu\", big_skip=True, pos_embed=\"latlon\", use_mlp=True,\n                                            normalization_layer=\"instance_norm\", hard_thresholding_fraction=hard_thresholding_fraction,\n                                            mlp_ratio = 2.).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\nscheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-5)\nprint(sum(p.numel() for p in model.parameters()))\n\n4122864\n\n\n\n# it takes roughly 75 sec per epoch to train the original LUCIE model on a single V100\n# checkpoint saved at epoch 340 is available in `/g/data/dk92/notebooks/examples-aiml/lucie/checkpoints/`\nnepochs=3\ntrain_model(model, train_loader, optimizer, scheduler=scheduler, nepochs=nepochs)\n\n--------------------------------------------------------------------------------\n18:39:35: epoch 0 start\nusing scheduler: current learning rate = [9.998026259498021e-05]\n\n\n/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n/opt/conda/envs/mlenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:990: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  _warn_get_lr_called_within_step(self)\n\n\n18:40:49: Epoch 0 summary:\ntime taken: 73.2493634223938\nnsamples / sec: 218.43193240787235\naverage training loss: 0.568795047662941\n--------------------------------------------------------------------------------\n18:40:49: epoch 1 start\nusing scheduler: current learning rate = [9.993093369094605e-05]\n18:42:02: Epoch 1 summary:\ntime taken: 73.02629399299622\nnsamples / sec: 219.0991644945658\naverage training loss: 0.44222918830883634\n--------------------------------------------------------------------------------\n18:42:02: epoch 2 start\nusing scheduler: current learning rate = [9.986190524833163e-05]\n18:43:15: Epoch 2 summary:\ntime taken: 72.95869874954224\nnsamples / sec: 219.30215689462784\naverage training loss: 0.4068589104065609\n\n\n\nSuggested Exercises\n\nplot the learning curve of the training: y-axis: average training loss, x-axis: epoch.\nmodify rollout_clim and true_clim to be the latitude-weighted global mean and re-run the training\ntry expand the region where the added spectral loss covers and re-run the training.\n\n\n\nThe END",
    "crumbs": [
      "Home",
      "Dlweather",
      "Lucie - Replicate Training"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html",
    "href": "tutorials/use-jupyterlab.html",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Create an NCI account using institution email\nJoin below NCI projects\n\nvp91:NCI Training Project # Launch JupyterLab Session on ARE\n\n\n\n\nOpen https://are.nci.org.au in a new tab, and log in with your NCI account, NOT email. You will see a dashboard page as shown after login. Each App will be run as an interactive session/job connected to Gadi. In this tutorial we will start a JupyterLab session. \n\n\n\nClick on the JupyterLab tile from the Dashboard, this will open the setting page for a new Jupyter session. We will fill in circled fields next.\n\n\n\n\nSetting form has free text input boxes. So we can copy the exact text values below and paste over to the form.\nWalltime (hours) 3\nQueue normal\nCompute Size small\nProject vp91\nStorage gdata/vp91+scratch/vp91\n\nInput boxes can also be used as dropdown selection. If you need to use different NCI projects when working on your project you might find the dropdown function helpful.\n\n\n\nClick on Show advanced settings at the end of page. This will expand the form with extra fields.  We will fill in selected fields with these values: Some field names are similar, check the field names you are editing are correct.\nModules python3/3.11.0 cuda/12.8.0\nPython or Conda virtual environment base /scratch/data/vp91/Training-Venvs/intro-to-dask\n\n\n\nAdvanced settings section\n\n\n\n\n\nMake sure the setting fields and values are filled correctly, and then click on the Launch button at the bottom of the setting page.\nNow you will see the green Session was successfully created message at the top, and Queued status is shown on the right side of the JupyterLab session block as shown below. Wait for the session to start. The wait time depends on the number of cores as well as time requested. \n\n\n\nOnce the requested resources are allocated, the session will start and the status will change to Running. The Jupyter path should start with the virtual environment base value used in settings, then point to /bin/jupyter.  Confirm the Jupyter path of your session is correct, and click on Open JupyterLab. This will open a new browser tab with JupyterLab interface. \nCongratulations, you are all set for the workshop!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#login-to-are",
    "href": "tutorials/use-jupyterlab.html#login-to-are",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Open https://are.nci.org.au in a new tab, and log in with your NCI account, NOT email. You will see a dashboard page as shown after login. Each App will be run as an interactive session/job connected to Gadi. In this tutorial we will start a JupyterLab session.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#new-jupyterlab-session",
    "href": "tutorials/use-jupyterlab.html#new-jupyterlab-session",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Click on the JupyterLab tile from the Dashboard, this will open the setting page for a new Jupyter session. We will fill in circled fields next.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#basic-settings",
    "href": "tutorials/use-jupyterlab.html#basic-settings",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Setting form has free text input boxes. So we can copy the exact text values below and paste over to the form.\nWalltime (hours) 3\nQueue normal\nCompute Size small\nProject vp91\nStorage gdata/vp91+scratch/vp91\n\nInput boxes can also be used as dropdown selection. If you need to use different NCI projects when working on your project you might find the dropdown function helpful.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#advanced-settings",
    "href": "tutorials/use-jupyterlab.html#advanced-settings",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Click on Show advanced settings at the end of page. This will expand the form with extra fields.  We will fill in selected fields with these values: Some field names are similar, check the field names you are editing are correct.\nModules python3/3.11.0 cuda/12.8.0\nPython or Conda virtual environment base /scratch/data/vp91/Training-Venvs/intro-to-dask\n\n\n\nAdvanced settings section",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#launch-session",
    "href": "tutorials/use-jupyterlab.html#launch-session",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Make sure the setting fields and values are filled correctly, and then click on the Launch button at the bottom of the setting page.\nNow you will see the green Session was successfully created message at the top, and Queued status is shown on the right side of the JupyterLab session block as shown below. Wait for the session to start. The wait time depends on the number of cores as well as time requested.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#open-jupyterlab",
    "href": "tutorials/use-jupyterlab.html#open-jupyterlab",
    "title": "Use JupyterLab on Gadi",
    "section": "",
    "text": "Once the requested resources are allocated, the session will start and the status will change to Running. The Jupyter path should start with the virtual environment base value used in settings, then point to /bin/jupyter.  Confirm the Jupyter path of your session is correct, and click on Open JupyterLab. This will open a new browser tab with JupyterLab interface. \nCongratulations, you are all set for the workshop!",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#unable-to-log-in",
    "href": "tutorials/use-jupyterlab.html#unable-to-log-in",
    "title": "Use JupyterLab on Gadi",
    "section": "Unable to Log In",
    "text": "Unable to Log In\nError:Bad Request: Requested resource does not exist.\nPossible Cause: This error is often caused by issues with browser cookies or cache.\nSolution: Open another tab and log in again, or try using incognito mode.\nError:We are sorry, but something went wrong.\nPossible Cause: Exceeding the /home file system quota on Gadi.\nDiagnosis: You will be able to confirm this by executing the quota -s command on Gadi’s login node. If this is the case, see the usage of Gadi’s HOME using the du -h --max-depth=1 ~ command.\nSolution: Deleting/moving files from your Gadi’s HOME directory to keep the usage below the quota.\nError:Web application could not be started by the Phusion Passenger(R) application server...\nPossible Cause: Exceeding the /home file system quota on Gadi.\nDiagnosis: You will be able to confirm this by executing the quota -s command on Gadi’s login node. If this is the case, see the usage of Gadi’s HOME using the du -h --max-depth=1 ~ command.\nSolution: Deleting/moving files from your Gadi’s HOME directory to keep the usage below the quota.\nError:Web application could not be started by the Phusion Passenger(R) application server...\nPossible Cause: Exceeding the /home file system quota on Gadi.\nDiagnosis: You will be able to confirm this by executing the quota -s command on Gadi’s login node. If this is the case, see the usage of Gadi’s HOME using the du -h --max-depth=1 ~ command.\nSolution: Deleting/moving files from your Gadi’s HOME directory to keep the usage below the quota.\n## Unable to Launch Session Error: qsub: Error: You are not a member of project vp91. You must be a member of a project to submit a job under that project.\nPossible Cause: You are not a member of the project used, or if you recently joined the project and system is syncing account status.\nSolution: Wait for 20 minutes and try again.\nError: Failed to submit session with the following error: usage: qsub [-a date_time]… If this job failed to submit because of an invalid job name please ask your administrator to configure OnDemand to set the environment variable OOD_JOB_NAME_ILLEGAL_CHARS.\nPossible Cause: Special characters are submitted into the setting form.\nSolution: Check the setting page values or manually type the values into the form.\nError:Disk quota exceeded @ dir_s_mkdir - /home/&lt;institution_code&gt;/&lt;username&gt;/ondemand/data/sys/dashboard/batch_connect/sys/desktop_vnc/ncigadi/output/&lt;session_ID&gt;\nPossible Cause: Exceeding the /home file system quota on Gadi.\nDiagnosis: You will be able to confirm this by executing the quota -s command on Gadi’s login node. If this is the case, see the usage of Gadi’s HOME using the du -h --max-depth=1 ~ command.\nSolution: Deleting/moving files from your Gadi’s HOME directory to keep the usage below the quota.\nError: unix listener: cannot bind to path /home/&lt;institution_code&gt;/&lt;username&gt;/.ssh/&lt;session_ID&gt;: No such file or directory. Your connection to the remote server has been terminated. Possible Cause: SSH folder is not properly setup in your account. Diagnosis: You will be able to confirm this by executing the ls -lah ~ command on Gadi’s login node. If there is no .ssh in the output or the folder permission is not correct, follow below steps to resolve the issue. Solution: 1. Login to Gadi terminal 2. Execute the following commands:\n2. **mkdir -p ~/.ssh**\n2. **chmod 700 ~/.ssh**",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#check-debug-log-for-other-issues",
    "href": "tutorials/use-jupyterlab.html#check-debug-log-for-other-issues",
    "title": "Use JupyterLab on Gadi",
    "section": "Check Debug Log for Other Issues",
    "text": "Check Debug Log for Other Issues\nIf your issue is not solved above, please follow these instructions to check the session log or report the issue: 1. Go to my interactive sessions by clicking one of the buttons on the page: \n\nOn the Session block, click Debug Log link  ## Report Issue to Helpdesk\nCopy Debug Log link (if job is still running then use the Session id link).\nSend a new email to help@nci.org.au, include:\n\nSubject: “ARE” and short description of issue\nBody:\n\nA more detailed issue description.\nDebug Log/Session id Link: The link you copied.\nOperating System: e.g. Windows 11, MacOSX 13, Debian Linux etc.\nBrowser: e.g. Firefox 102, Chrome 103, Edge 103\nConnection: e.g. Wired network at ANU, Wireless at Home etc.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#interface",
    "href": "tutorials/use-jupyterlab.html#interface",
    "title": "Use JupyterLab on Gadi",
    "section": "Interface",
    "text": "Interface\nThe JupyterLab workspace consists of a main work area containing tabs of documents and activities, a collapsible left sidebar, and a menu bar.\nThe left sidebar contains a file browser, the list of running kernels and terminals, the Dask extension of JupyterLab, Table of Contentions and Extension Manager.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#running-notebook",
    "href": "tutorials/use-jupyterlab.html#running-notebook",
    "title": "Use JupyterLab on Gadi",
    "section": "Running Notebook",
    "text": "Running Notebook\nDouble-click on a notebook (.ipynb) file to open it in the main area. Selected cell is highlighted in blue. Run Selected Cell\nPress: Shift + Enter or Click: the run button on toolbar",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/use-jupyterlab.html#server-connection-error",
    "href": "tutorials/use-jupyterlab.html#server-connection-error",
    "title": "Use JupyterLab on Gadi",
    "section": "Server Connection Error",
    "text": "Server Connection Error\nError:\"Server Connection Error. A connection to the Jupyter server could not be established. JupyterLab will continue trying to reconnect. Check your network connection or Jupyter server configuration.\" Possible Cause: Reached requested job Walltime or exceeding requested JOBFS size. Diagnosis: You will be able to confirm this by checking session log file:$HOME/ondemand/data/sys/dashboard/batch_connect/sys/jupyter/ncigadi/output/&lt;session_ID&gt;/output.logon Gadi. Solution: Re-launching the JupyterLab session by either requesting more Walltime or JOBFS (available under the “Advanced options …”), based on the cause of the issue. ## Saving File Error\nError: \"Unexpected error while saving file: … [Errno 13] Permission denied: '…' \" Possible Cause: This error occurs when a file with the same name already exists and might be owned by another user. Diagnosis: Check the file directory on the left panel of JupyterLab page that you are saving the file in your own folder, not other users. Suggestion: Rename the file and try saving it again.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Use JupyterLab on Gadi"
    ]
  },
  {
    "objectID": "tutorials/accounts.html",
    "href": "tutorials/accounts.html",
    "title": "Accounts",
    "section": "",
    "text": "New and existing NCI users with stakeholder affiliation can register for an account or new project at our MyNCI portal.\nLead Chief Investigators can register a new project, and the Scheme Manager for the nominated allocation scheme will receive the new proposal in email. The Scheme Manager will review and decide if the proposal will be approved.\nProspective stakeholders can register interest in obtaining NCI services via NCI user support at help@nci.org.au. Please include contact details and a basic description of your requirements in your enquiry email.\n\nAccess Scheme\nNational Computational Merit Allocation Scheme The National Computational Merit Allocation Scheme (NCMAS) provides researchers with annual access to Australia’s major national computational facilities, namely NCI and the Pawsey Supercomputing Research Centre, through a merit-based selection process. In total, NCMAS distributes around 700 million units of computing time to meritorious applicants from around Australia. The main call for applications is made annually in August-October to determine allocations for the following calendar year. Information and application materials are available through the NCMAS portal.\nOther access schemes and methods can be found here.\nNote that your account needs to be linked to a project to access NCI resources. Resources at NCI are allocated to projects and not to individual users.\n\n\nCreate User Account\n\nGo to https://my.nci.org.au/mancini\n\nClick on the Sign up here link and complete the registration form. Note: you must provide a current institutional email address (Gmail, Hotmail, etc are not accepted). Mobile phone number is strongly encouraged to easily reset your password.\n\nSelect the option to join an existing project or propose a new project at Step 3 of the form. Note: A new project proposal to be assessed by a Scheme Manager to determine if they will grant your project time.\n\nClick “Finish” on the final page of the form to complete your registration request.\n\nYour username will become active when a project Lead CI approves your request to join their project, or when a Scheme Manager approves your new project proposal. You will receive a confirmation email.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Accounts"
    ]
  }
]